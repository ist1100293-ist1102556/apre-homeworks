\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{nicematrix}
\usepackage[colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{listings}

\setlength{\droptitle}{-6em}

\newcommand{\info}[2]{\frac{#1}{#2}\log\bigl( \frac{#1}{#2} \bigr)}

\begin{document}

\center
Aprendizagem 2023\\
Homework I -- Group 016\\
(ist1100293, ist1102556)\vskip 1cm

\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep]
\item Complete the given decision tree using Information gain with Shannon entropy ($\log_2$).
Consider that: i) a minimum of 4 observations is required to split an internal node, and
ii) decisions by ascending alphabetic order should be placed in case of ties.
 
\begin{center}
    \includegraphics[scale=0.3]{images/decision-tree.png}
\end{center}

\paragraph{For $y_1 > 0.4$}

\begin{equation}
    \begin{split}
        H(y_\textrm{out}) & = -\sum_{v \in y_\textrm{out}}\textrm{p}(v)\log(\textrm{p}(v)) \\
        & = -\info{3}{7} - \info{2}{7} - \info{2}{7} \\
        & \approx 1.5567
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \textrm{IG}(y_\textrm{out}, y_2) &= H(y_\textrm{out}) - H(y_\textrm{out}|y_2) \\
        & = H(y_\textrm{out}) - \sum_{v \in y_2}\textrm{p}(v)H(y_\textrm{out}|y_2=v) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{3}{7}H(y_\textrm{out}|y_2=0) + \frac{2}{7}H(y_\textrm{out}|y_2=1) + \frac{2}{7}H(y_\textrm{out}|y_2=2)\Biggr) \\
        & = H(y_\textrm{out}) - \Biggl( -\frac{3}{7}\biggl( \info{1}{3} + \info{1}{3} + \info{1}{3} \biggr) \\
        & - \frac{2}{7}\biggl( \info{1}{2} + \info{1}{2} \biggr) - \frac{2}{7} \cdot 0 \Biggr) \\
        & \approx 0.5917
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \textrm{IG}(y_\textrm{out}, y_3) &= H(y_\textrm{out}) - H(y_\textrm{out}|y_3) \\
        & = H(y_\textrm{out}) - \sum_{v \in y_3}\textrm{p}(v)H(y_\textrm{out}|y_3=v) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{1}{7}H(y_\textrm{out}|y_3=0) + \frac{2}{7}H(y_\textrm{out}|y_3=1) + \frac{4}{7}H(y_\textrm{out}|y_3=2)\Biggr) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{1}{7} \cdot 0 + \frac{2}{7} \cdot 1 + \frac{4}{7} \cdot 1  \Biggr) \\
        & = H(y_\textrm{out}) - \frac{6}{7} \\
        & \approx 0.6996
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \textrm{IG}(y_\textrm{out}, y_4) &= H(y_\textrm{out}) - H(y_\textrm{out}|y_4) \\
        & = H(y_\textrm{out}) - \sum_{v \in y_4}\textrm{p}(v)H(y_\textrm{out}|y_4=v) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{2}{7}H(y_\textrm{out}|y_4=0) + \frac{3}{7}H(y_\textrm{out}|y_4=1) + \frac{2}{7}H(y_\textrm{out}|y_4=2)\Biggr) \\
        & = H(y_\textrm{out}) - \Biggl( -\frac{2}{7} \cdot 1 - \frac{3}{7}\Bigl( \info{1}{3} + \info{2}{3} \Bigr) - \frac{2}{7} \cdot 1\Biggr)\\
        & \approx 0.5917
    \end{split}
\end{equation}

\paragraph{} For this node $y_3$ has the highest information gain, so we choose it to split the node.

\paragraph{For $y_1 > 0.4 \wedge y_3=2$}

\begin{equation}
    \begin{split}
        H(y_\textrm{out}) & = -\sum_{v \in y_\textrm{out}}\textrm{p}(v)\log(\textrm{p}(v)) \\
        & = -\info{1}{2} - \info{1}{2} \\
        & = 1
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \textrm{IG}(y_\textrm{out}, y_2) &= H(y_\textrm{out}) - H(y_\textrm{out}|y_2) \\
        & = H(y_\textrm{out}) - \sum_{v \in y_2}\textrm{p}(v)H(y_\textrm{out}|y_2=v) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{1}{2}H(y_\textrm{out}|y_2=0) + \frac{1}{4}H(y_\textrm{out}|y_2=1) + \frac{1}{4}H(y_\textrm{out}|y_2=2)\Biggr) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{1}{2} \cdot 0 + \frac{1}{4} \cdot 0 + \frac{1}{4} \cdot 0  \Biggr) \\
        & = H(y_\textrm{out}) - 0 \\
        & = 1
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \textrm{IG}(y_\textrm{out}, y_4) &= H(y_\textrm{out}) - H(y_\textrm{out}|y_4) \\
        & = H(y_\textrm{out}) - \sum_{v \in y_4}\textrm{p}(v)H(y_\textrm{out}|y_4=v) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{1}{2}H(y_\textrm{out}|y_3=0) + \frac{1}{4}H(y_\textrm{out}|y_3=1) + \frac{1}{4}H(y_\textrm{out}|y_3=2)\Biggr) \\
        & = H(y_\textrm{out}) - \Biggl( \frac{1}{2} \Bigl( - \info{1}{2} - \info{1}{2} \Bigr) + \frac{1}{4} \cdot 0 + \frac{1}{4} \cdot 0  \Biggr) \\
        & = H(y_\textrm{out}) - \frac{1}{2} \\
        & = \frac{1}{2}
    \end{split}
\end{equation}

\paragraph{} For this node $y_2$ has the highest information gain, so we choose it to split the node.

\item Draw the training confusion matrix for the learnt decision tree.

\begin{center}

    \begin{table}[htbp]
    \centering
    \begin{NiceTabular}{*{4}{c}*{6}{p{1cm}}}[hvlines]
    \Block{2-2}{} & & \Block{1-3}{Real}\\
    & & A & B & C\\
    \Block{3-1}{Predicted}& A & 4 & 1 & 0\\
    & B & 0 & 2 & 0\\
    & C & 0 & 1 & 4\\
    \end{NiceTabular}
    \end{table}

\end{center}



\item Identify which class has the lowest training F1 score.

\paragraph{Class A}

\begin{equation}
    P = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FP}} = \frac{4}{4+1} = \frac{4}{5}
\end{equation}

\begin{equation}
    R = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FN}} = \frac{4}{4+0} = 1
\end{equation}

\begin{equation}
    \frac{1}{F_A} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R}) = \frac{1}{2}\cdot(\frac{5}{4}+1) = \frac{9}{8}
\end{equation}

\begin{equation}
    F_A = \frac{8}{9}
\end{equation}


\paragraph{Class B}

\begin{equation}
    P = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FP}} = \frac{2}{2+0} = 1
\end{equation}

\begin{equation}
    R = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FN}} = \frac{2}{2+2} = \frac{1}{2}
\end{equation}

\begin{equation}
    \frac{1}{F_B} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R}) = \frac{1}{2}\cdot(1+2) = \frac{3}{2}
\end{equation}

\begin{equation}
    F_B = \frac{2}{3}
\end{equation}

\paragraph{Class C}

\begin{equation}
    P = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FP}} = \frac{4}{4+1} = \frac{4}{5}
\end{equation}

\begin{equation}
    R = \frac{\textrm{TP}}{\textrm{TP} + \textrm{FN}} = \frac{4}{4+0} = 1
\end{equation}

\begin{equation}
    \frac{1}{F_A} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R}) = \frac{1}{2}\cdot(\frac{5}{4}+1) = \frac{9}{8}
\end{equation}

\begin{equation}
    F_A = \frac{8}{9}
\end{equation}

\paragraph{} The class B has the lowest training $F_1$ score.

\item Considering $y_2$ to be ordinal, assess if $y_1$ and $y_2$ are correlated using the Spearman coefficient.
\begin{center}
    
    \begin{tabular}{|c | c | c | c | c|}
        \hline
        D & $y_1$ & $r_{y_1}$ & $y_2$ & $r_{y_2}$\\
        \hline
        $x_1$    & 0.24 & 3  & 1 & 8 \\
        $x_2$    & 0.06 & 2  & 2 & 11 \\
        $x_3$    & 0.04 & 1  & 0 & 3.5 \\
        $x_4$    & 0.36 & 5  & 0 & 3.5 \\
        $x_5$    & 0.32 & 4  & 0 & 3.5 \\
        $x_6$    & 0.68 & 10 & 2 & 11 \\
        $x_7$    & 0.9  & 12 & 0 & 3.5 \\
        $x_8$    & 0.76 & 11 & 2 & 11 \\
        $x_9$    & 0.46 & 7  & 1 & 8 \\
        $x_{10}$ & 0.62 & 9  & 0 & 3.5 \\
        $x_{11}$ & 0.44 & 6  & 1 & 8 \\
        $x_{12}$ & 0.52 & 8  & 0 & 3.5 \\
        \hline
    \end{tabular}
\end{center}

    \begin{equation}
        \begin{split}
            \textrm{Spearman}(y_1, y_2) & = \textrm{Pearson}(r_{y_1}, r_{y_2}) \\
            & = \frac{\sum (r_{y_1}-\overline{r_{y_1}})(r_{y_2}-\overline{r_{y_1}})}{\sqrt[]{\sum (r_{y_1}-\overline{r_{y_1}})^2}\sqrt[]{\sum (r_{y_2}-\overline{r_{y_2}})^2}} \\
            & = \frac{\sum r_{y_1}r_{y_2} - \frac{\sum r_{y_1} \sum r_{y_2}}{n}}{\sqrt{(\sum r_{y_1}^2-\frac{(\sum r_{y_1})^2}{n})(\sum r_{y_2}^2-\frac{(\sum r_{y_2})^2}{n })}}
        \end{split}
    \end{equation}

    \paragraph{}We have: $\sum r_{y_1}r_{y_2} = 517.5$, $\sum r_{y_1} = 78$, $\sum r_{y_2} = 78$, $\sum r_{y_1}^2 = 650$, $\sum r_{y_2}^2 = 628.5$. Which gives: 
        
    \begin{equation}
        \textrm{Spearman}(y_1, y_2) = 0.0797
    \end{equation}

    \paragraph{}This indicates that $y_1$ and $y_2$ have a very weak correlation, to the point that it is negligible.

    \item Draw the class-conditional relative histograms of $y_1$ using 5 equally spaced bins in [0,1].
    Challenge: find the root split using the discriminant rules from these empirical distributions.    

    \begin{center}
        \includegraphics[scale=1]{images/histogram.png}
    \end{center}

    \paragraph{} By choosing the class with the bigger probability in each bin we get the following decision tree:

    \begin{center}
        \includegraphics[scale=0.5]{images/small-decision-tree.png}
    \end{center}

\end{enumerate}

\vskip 1cm

\large{\textbf{Part II}: Programming}\normalsize

\paragraph{} To answer the following questions, consider using the sklearn API documentation and the notebooks in
the course webpage as guidance. Show in your PDF report both the code and the corresponding results.
Consider the column\_diagnosis.arff data available at the homework tab, comprising 6 biomechanical
features to classify 310 orthopaedic patients into 3 classes (normal, disk hernia, spondilolysthesis). 

\begin{lstlisting}
    import pandas as pd
    from scipy.io.arff import loadarff

    data = loadarff('data/column_diagnosis.arff')
    df = pd.DataFrame(data[0])
    df['class'] = df['class'].str.decode('utf-8')
\end{lstlisting}

\begin{enumerate}
    \item Apply f\_classif from sklearn to assess the discriminative power of the input variables. Identify the input variable with the highest and lowest discriminative power. Plot the class-conditional probability density functions of these two input variables.
    
    \begin{lstlisting}
        from sklearn.feature_selection import f_classif

        X, y = df.drop('class', axis=1), df['class']

        scores = list(zip(X.columns.values, f_classif(X, y)[0]))
        scores.sort(key=(lambda x: x[1]))

        print(scores)
    \end{lstlisting}
    Output:
    \begin{lstlisting}
        [('pelvic_radius', 16.86693475538006),
        ('pelvic_tilt', 21.299194328989202),
        ('sacral_slope', 89.64395329777439),
        ('pelvic_incidence', 98.53970917437658), 
        ('lumbar_lordosis_angle', 114.98284047330131),
        ('degree_spondylolisthesis', 119.12288060759764)]
    \end{lstlisting}
    
    The variable with the highest is degree\_spondylolisthesis and the lowest is pelvic\_radius.

    To plot:

    \begin{lstlisting}
import matplotlib.pyplot as plt
import seaborn as sns

plot = sns.FacetGrid(df, hue="class")
plot.map(sns.kdeplot, "pelvic_radius").add_legend()

plot = sns.FacetGrid(df, hue="class")
plot.map(sns.kdeplot, "degree_spondylolisthesis").add_legend()

plt.show()
    \end{lstlisting}

    \begin{center}
        \includegraphics[]{images/plot1.png}
        \includegraphics[]{images/plot2.png}
    \end{center}

\item Using a stratified 70-30 training-testing split with a fixed seed (random\_state=0), assess in a
single plot both the training and testing accuracies of a decision tree with depth limits in
{1,2,3,4,5,6,8,10} and the remaining parameters as default.
[optional] Note that split thresholding of numeric variables in decision trees is non-deterministic
in sklearn, hence you may opt to average the results using 10 runs per parameterization.

    \begin{lstlisting}[basicstyle=\tiny]
from sklearn.model_selection import train_test_split
from sklearn import tree, metrics

n_runs = 10
depth_limits = [1, 2, 3, 4, 5, 6, 8, 10]

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0, stratify=y)

# Generates a list composed of 8 lists (one for each depth limit) each containing 10 instances
# of a decision tree trained with the training data.
predictors = [[tree.DecisionTreeClassifier(max_depth=k).fit(X_train, y_train) for i in range(n_runs)] for k in depth_limits]

# Computes the individual training and testing accuracies for each of the predictors
individual_training_accuracies = map(lambda depth: [metrics.accuracy_score(y_train, predictor.predict(X_train)) for predictor in depth], \
                                    predictors)
individual_testing_accuracies = map(lambda depth: [metrics.accuracy_score(y_test, predictor.predict(X_test)) for predictor in depth], \
                                    predictors)

# Averages the training and testing accuracies for each depth limit
training_accuracies = map(lambda depth: sum(depth)/len(depth), individual_training_accuracies)
testing_accuracies = map(lambda depth: sum(depth)/len(depth), individual_testing_accuracies)

# Creates a dataframe with the results in order to plot
testing_accuracies_rows = list(zip(depth_limits, testing_accuracies, ['testing' for i in range(8)]))
training_accuracies_rows = list(zip(depth_limits, training_accuracies, ['training' for i in range(8)]))
data = pd.DataFrame(testing_accuracies_rows + training_accuracies_rows, columns=['depth_limit', 'accuracy', 'type'])

sns.pointplot(x="depth_limit", y="accuracy", data=data, hue='type')
plt.legend(bbox_to_anchor=(1, 1), loc=2)  
plt.grid()
plt.show()
    \end{lstlisting}

    Output

    \begin{center}
        \includegraphics[scale=0.6]{images/plot3.png}
    \end{center}

    \item Comment on the results, including the generalization capacity across settings.

    The model seems to be overfitting when the depth limit surpasses 4, since the training accuracy continues to grow (until it reaches close to 100% at depth 8) while the testing accuracy actually tends to diminuish. The model seems to lack generalization capacity for depth limits bigger than 4.

    Although, when the limit is 1, there is a simillar value to testing and training accuracies, that seems to be the case because both of them are pretty low. That fact gets highlighted for limits 2 and 3, where the gap between the accuracies grows.

    In conclusion, to get the best case of generalization (avoiding underfitting and overfitting) we would chose a depth limit of 4 for a decision tree trained using this data set.


    \vskip 20cm

    \item To deploy the predictor, a healthcare team opted to learn a single decision tree
    (random\_state=0) using all available data as training data, and further ensuring that each leaf has
    a minimum of 20 individuals in order to avoid overfitting risks.

    \begin{enumerate}
        \item Plot the decision tree.
        \begin{lstlisting}[basicstyle=\tiny]
healthcare_predictor = tree.DecisionTreeClassifier(random_state=0, min_samples_leaf=20)
healthcare_predictor.fit(X, y)

figure = plt.figure(figsize=(20, 15))
tree.plot_tree(healthcare_predictor, feature_names=list(X.head(0)), class_names=sorted(list(set(y.values))), impurity=False)
plt.show()
        \end{lstlisting}

        Output:

        \begin{center}
            \includegraphics*[scale=0.35]{images/plot4.png}
        \end{center}
        
        \item Characterize a hernia condition by identifying the hernia-conditional associations.
        The hernia condition can be characterized by the following conditions: spondylolisthesis degree lower or equal to 16.079, sacral slope lower or equal to 28.136 or between 28.136 (excluding) and 40.149 when the pelvic radius is lower or equal to 117.36.

        In resume, the following conditions lead to a Hernia diagnosis:

        \begin{enumerate}
            \item spondylolisthesis degree $\leq 16.079$ $\land$ sacral slope $\leq 28.136$
            \item spondylolisthesis degree $\leq 16.079$ $\land$ $28.136 <$ sacral slope $\leq 40.149$ $\land$ pelvic radius $\leq 117.36$
        \end{enumerate}
            
    \end{enumerate}
\end{enumerate}

\end{document}
