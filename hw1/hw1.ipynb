{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "To answer the following questions, consider using the sklearn API documentation and the notebooks in\n",
    "the course webpage as guidance. Show in your PDF report both the code and the corresponding results.\n",
    "Consider the column_diagnosis.arff data available at the homework tab, comprising 6 biomechanical\n",
    "features to classify 310 orthopaedic patients into 3 classes (normal, disk hernia, spondilolysthesis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "data = loadarff('data/column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Apply f_classif from sklearn to assess the discriminative power of the input variables.\n",
    "Identify the input variable with the highest and lowest discriminative power.\n",
    "Plot the class-conditional probability density functions of these two input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X, y = df.drop('class', axis=1), df['class']\n",
    "\n",
    "scores = list(zip(X.columns.values, f_classif(X, y)[0]))\n",
    "scores.sort(key=(lambda x: x[1]))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Meter histogramas ou densidade estimada (kernel)\n",
    "\n",
    "plot = sns.FacetGrid(df, hue=\"class\")\n",
    "plot.map(sns.kdeplot, \"pelvic_radius\").add_legend()\n",
    "  \n",
    "plot = sns.FacetGrid(df, hue=\"class\")\n",
    "plot.map(sns.kdeplot, \"degree_spondylolisthesis\").add_legend()\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Using a stratified 70-30 training-testing split with a fixed seed (random_state=0), assess in a\n",
    "single plot both the training and testing accuracies of a decision tree with depth limits in\n",
    "{1,2,3,4,5,6,8,10} and the remaining parameters as default.\n",
    "\n",
    "[optional] Note that split thresholding of numeric variables in decision trees is non-deterministic\n",
    "in sklearn, hence you may opt to average the results using 10 runs per parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0, stratify=y)\n",
    "\n",
    "depth_limits = [1, 2, 3, 4, 5, 6, 8, 10]\n",
    "training_accuracies = [0 for i in range(8)]\n",
    "testing_accuracies = [0 for i in range(8)]\n",
    "\n",
    "for i in range(10):\n",
    "    predictors = [tree.DecisionTreeClassifier(max_depth=k).fit(X_train, y_train) for k in depth_limits]\n",
    "\n",
    "    j = 0\n",
    "    for predictor in predictors:\n",
    "        training_accuracies[j] += metrics.accuracy_score(y_train, predictor.predict(X_train))\n",
    "        testing_accuracies[j] += metrics.accuracy_score(y_test, predictor.predict(X_test))\n",
    "        j += 1\n",
    "\n",
    "training_accuracies = [0.1*x for x in training_accuracies]\n",
    "testing_accuracies = [0.1*x for x in testing_accuracies]\n",
    "\n",
    "training_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_accuracies_rows = list(zip(depth_limits, testing_accuracies, ['testing' for i in range(8)]))\n",
    "training_accuracies_rows = list(zip(depth_limits, training_accuracies, ['training' for i in range(8)]))\n",
    "\n",
    "\n",
    "data = pd.DataFrame(testing_accuracies_rows + training_accuracies_rows, columns=['depth_limit', 'accuracy', 'type'])\n",
    "sns.pointplot(x=\"depth_limit\", y=\"accuracy\", data=data, hue='type')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Comment on the results, including the generalization capacity across settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be overfitting when the depth limit surpasses 4, since the training accuracy continues to grow (until it reaches close to 100% at depth 8) while the testing accuracy actually tends to diminuish. The model seems to lack generalization capacity for depth limits bigger than 4.\n",
    "\n",
    "Although, when the limit is 1, there is a simillar value to testing and training accuracies, that seems to be the case because both of them are pretty low. That fact gets highlighted for limits 2 and 3, where the gap between the accuracies grows.\n",
    "\n",
    "In conclusion, to get the best case of generalization (avoiding underfitting and overfitting) we would chose a depth limit of 4 for a decision tree trained using this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) To deploy the predictor, a healthcare team opted to learn a single decision tree\n",
    "(random_state=0) using all available data as training data, and further ensuring that each leaf has\n",
    "a minimum of 20 individuals in order to avoid overfitting risks.\n",
    "i. Plot the decision tree.\n",
    "ii. Characterize a hernia condition by identifying the hernia-conditional associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_predictor = tree.DecisionTreeClassifier(random_state=0, min_samples_leaf=20)\n",
    "healthcare_predictor.fit(X, y)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 15))\n",
    "tree.plot_tree(healthcare_predictor, feature_names=list(X.head(0)), class_names=sorted(list(set(y.values))), impurity=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Check if they want the whole path, including duplicates)\n",
    "\n",
    "The hernia condition can be characterized by the following conditions: spondylolisthesis degree lower or equal to 16.079, sacral slope lower or equal to 28.136 or between 28.136 (excluding) and 40.149 when the pelvic radius is lower or equal to 117.36.\n",
    "\n",
    "In resume, the following conditions lead to a Hernia diagnosis:\n",
    "\n",
    "- spondylolisthesis degree $\\leq 16.079$ $\\land$ sacral slope $\\leq 28.136$\n",
    "- spondylolisthesis degree $\\leq 16.079$ $\\land$ $28.136 \\leq$ sacral slope $\\leq 40.149$ $\\land$ pelvic radius $\\leq 117.36$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
