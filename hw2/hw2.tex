\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{nicematrix}
\usepackage[colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathtools}

\setlength{\droptitle}{-6em}
\begin{document}

\newcommand{\prob}{\textrm{P}}
\newcommand{\ind}{\perp\!\!\!\!\!\perp} 
\newcommand{\notind}{\not\perp\!\!\!\!\!\perp}
\newcommand{\defeq}{\vcentcolon=}

\center
Aprendizagem 2023\\
Homework II -- Group 016\\
(ist1100293, ist1102556)\vskip 1cm

\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep]

\item Consider $\mathbf{x}_1$-$\mathbf{x}_7$ to be training observations, $\mathbf{x}_8$-$\mathbf{x}_9$ to be testing observations, $y_1$-$y_5$ to be input
variables and $y_6$ to be the target variable.
Hint: you can use scipy.stats.multivariate\_normal for multivariate distribution calculus

    \begin{enumerate}
        \item Learn a Bayesian classifier assuming: i) ${y_1, y_2}$, ${y_3, y_4}$ and ${y_5}$ sets of independent
        variables (e.g., $y_1 \ind y_3$ yet $y_1 \notind y_2$), and ii) $y_1 \times y_2 \in \mathbb{R}^2$
        is normally distributed. Show all
        parameters (distributions and priors for subsequent testing).

        \paragraph{} Since the variables $y_1$-$y_5$ can be divided into the independent subsets: $\{y_1, y_2\}, \{y_3, y_4\}, \{y_5\}$, we have:

        \begin{multline}
            \prob(y_1 = x_1, y_2 = x_2, y_3 = x_3, y_4 = x_4, y_5 = x_5 | y_6 = C) = \\
            \prob(y_1 = x_1, y_2 = x_2 | y_6 = C)\prob(y_3 = x_3, y_4 = x_4|y_6 = C)\prob(y_5 = x_5|y_6 = C)
        \end{multline}

        \paragraph{Priors}
        \begin{equation}
            \prob(y_6 = A) = \frac{3}{7} \qquad \prob(y_6 = B) = \frac{4}{7}
        \end{equation}

        \paragraph{Y1 and Y2 Conditional PDF's}
        Since we are assuming that $y_1$ and $y_2$ are normally distributed, we will replace the conditional probabilities of $y_1$ and $y_2$ with the value of the conditional PDF of these variables.

        \begin{equation}
        \begin{split}
            \prob(y_1 = x_1, y_2 = x_2 | y_6 = C) &\defeq \textrm{N}(\mathbf{x} | \mu_C, \Sigma_C) \\
            &\defeq \frac{1}{(2\pi)^{m/2}\sqrt{|\Sigma_C|}}\exp\Bigl(-\frac{1}{2}(\mathbf{x}-\mu_C)^\intercal \Sigma_C^{-1} (\mathbf{x}-\mu_C)\Bigr)
        \end{split}
        \end{equation}

        Where $\mu_C$ is the class conditional multivariate average of the data,

        \begin{equation}
        \mu_A = \begin{bmatrix}
            \frac{0.24 + 0.16 + 0.32}{3} \\
            \frac{0.36 + 0.48 + 0.72}{3}
        \end{bmatrix} = \begin{bmatrix}
            0.24 \\
            0.52
        \end{bmatrix}
        \end{equation}

        \begin{equation}
            \mu_B =
            \begin{bmatrix}
                \frac{0.54 + 0.66 + 0.76 + 0.41}{4} \\
                \frac{0.11 + 0.39 + 0.28 + 0.53}{4}
            \end{bmatrix} =
            \begin{bmatrix}
                0.5925 \\
                0.3275
            \end{bmatrix}
        \end{equation}

        and $\Sigma_C$ is the class conditional covariance matrix of the data:

        \begin{equation}
            \Sigma_C =
            \begin{bmatrix}
                \textrm{var}(y_1)_C & \textrm{cov}(y_1, y_2)_C \\
                \textrm{cov}(y_1)_C & \textrm{var}(y_2)_C
            \end{bmatrix} 
        \end{equation}

        Where $\textrm{var}(y) = \frac{1}{N-1}\sum_{i = 0}^{N}(x_i - \bar{y})^2$ and $\textrm{cov}(y_1, y_2) = \frac{1}{N-1}\sum_{i = 0}^{N}(x_{1i} - \bar{y_1})(x_{2i} - \bar{y_2})$.
        So we have:

        \begin{equation}
        \begin{split}
            \textrm{var}(y_1)_A &= \frac{(0.24-0.24)^2 + (0.16-0.24)^2 + (0.32-0.24)^2}{2} = 0.0064 \\
            \textrm{var}(y_2)_A &= \frac{(0.36-0.52)^2 + (0.48-0.52)^2 + (0.72-0.52)^2}{2} = 0.0336 \\
            \textrm{cov}(y_1, y_2)_A &= \frac{(0.24-0.24)(0.36-0.52) + (0.16-0.24)(0.48-0.52)}{2}+ \\
            &+ \frac{(0.32-0.24)(0.72-0.52)}{2} = 0.0096
        \end{split}
        \end{equation}

        \begin{equation}
            \Sigma_A =
            \begin{bmatrix}
                \textrm{var}(y_1)_A & \textrm{cov}(y_1, y_2)_A \\
                \textrm{cov}(y_1)_A & \textrm{var}(y_2)_A
            \end{bmatrix}=
            \begin{bmatrix}
                0.0064 & 0.0096 \\
                0.0096 & 0.0363
            \end{bmatrix}
        \end{equation}

        \begin{equation}
            \Sigma_B =
            \begin{bmatrix}
                \textrm{var}(y_1)_B & \textrm{cov}(y_1, y_2)_B \\
                \textrm{cov}(y_1)_B & \textrm{var}(y_2)_B
            \end{bmatrix}=
            \begin{bmatrix}
                0.0229 & -0.0098 \\
                -0.0098 & 0.0315
            \end{bmatrix}
        \end{equation}

        \paragraph{Y3 and Y4 Conditional PMF's}

        \begin{equation}
        \begin{split}
            \prob(y_3 = 0, y_4 = 0 | y_6 = A) = 0 &\qquad \prob(y_3 = 0, y_4 = 0 | y_6 = B) = \frac{1}{2} \\
            \prob(y_3 = 0, y_4 = 1 | y_6 = A) = \frac{1}{3} &\qquad \prob(y_3 = 0, y_4 = 1 | y_6 = B) = \frac{1}{4} \\
            \prob(y_3 = 1, y_4 = 0 | y_6 = A) = \frac{1}{3} &\qquad \prob(y_3 = 1, y_4 = 0 | y_6 = B) = \frac{1}{4} \\
            \prob(y_3 = 1, y_4 = 1 | y_6 = A) = \frac{1}{3} &\qquad \prob(y_3 = 1, y_4 = 1 | y_6 = B) = 0
        \end{split}
        \end{equation}

        \paragraph{Y5 Conditional PMF's}

        \begin{equation}
        \begin{split}
            \prob(y_5 = 0 | y_6 = A) = \frac{1}{3} &\qquad \prob(y_5 = 0 | y_6 = B) = \frac{1}{4} \\
            \prob(y_5 = 1 | y_6 = A) = \frac{1}{3} &\qquad \prob(y_5 = 1 | y_6 = B) = \frac{1}{2} \\
            \prob(y_5 = 2 | y_6 = A) = \frac{1}{3} &\qquad \prob(y_5 = 2 | y_6 = B) = \frac{1}{4}
        \end{split}
        \end{equation}

        \paragraph{Whole Probabilities}

        (Even though you don't need the values of the whole probabilities (evidences) to classify according to MAP o ML, we will still compute those in case we need later.)

        \begin{equation}
            \prob(y_1 = x_1, y_2 = x_2) = \textrm{N}(\mathbf{x} | \mu, \Sigma)
        \end{equation}

        Where:
    
        \begin{equation}
            \mu = \begin{bmatrix}
                0.4414 \\
                0.4100
            \end{bmatrix} \qquad \Sigma = \begin{bmatrix}
                0.0491 & -0.0211 \\
                -0.211 & 0.0375
            \end{bmatrix}
        \end{equation}

        \begin{equation}
        \begin{split}
            \prob(y_3 = 0, y_4 = 0) = \frac{2}{7} &\qquad \prob(y_3 = 0, y_4 = 1) = \frac{2}{7} \\
            \prob(y_3 = 1, y_4 = 0) = \frac{2}{7} &\qquad \prob(y_3 = 1, y_4 = 1) = \frac{1}{7}
        \end{split}
        \end{equation}

        \begin{equation}
            \prob(y_5 = 0) = \frac{2}{7} \qquad \prob(y_5 = 1) = \frac{3}{7} \qquad \prob(y_5 = 2) = \frac{2}{7}
        \end{equation}

        \item Under a MAP assumption, classify each testing observation showing all your calculus.
    
        \item  Consider that the default decision threshold of $\theta = 0.5$ can be adjusted according to
    
            \[ 
            f(\mathbf{x}|\theta)= \left\{
            \begin{array}{ll}
                  \textrm{A} & \prob(\textrm{A}|\mathbf{x}) > \theta \\
                  \textrm{B} & \textrm{otherwise}
            \end{array} 
            \right. 
            \]

            Under a maximum likelihood assumption, what thresholds optimize testing accuracy?
    \end{enumerate}

    \item Let $y_1$ be the target numeric variable, $y_2$-$y_6$ be the input variables where $y_2$ is binarized under an
    equal-width (equal-range) discretization. For the evaluation of regressors, consider a 3-fold
    cross-validation over the full dataset ($\mathbf{x}_1$-$\mathbf{x}_9$) without shuffling the observations.

    \begin{enumerate}
        \item Identify the observations and features per data fold after the binarization procedure.
        \item  Consider a distance-weighted $k$NN with $k = 3$, Hamming distance ($d$), and $1/d$ weighting.
        Compute the MAE of this $k$NN regressor for the 1
        st iteration of the cross-validation (i.e. train
        observations have the lower indices).
    \end{enumerate}
\end{enumerate}

\vskip 1cm

\large{\textbf{Part II}: Programming and Critical Analysis}\normalsize

\paragraph{}Considering the column\_diagnosis.arff dataset available at the course webpage’s homework tab.
Using sklearn, apply a 10-fold stratified cross-validation with shuffling (random\_state=0) for the
assessment of predictive models along this section.

\begin{enumerate}[leftmargin=\labelsep]
    \item Compare the performance of $k$NN with $k = 5$ and naïve Bayes with Gaussian assumption
    (consider all remaining parameters for each classifier as sklearn's default):

    \begin{enumerate}
        \item Plot two boxplots with the fold accuracies for each classifier.
        \item Using scipy, test the hypothesis “$k$NN is statistically superior to naïve Bayes regarding
        accuracy”, asserting whether is true.
    \end{enumerate}

    \item Consider two $k$NN predictors with $k = 1$ and $k = 5$ (uniform weights, Euclidean distance,
    all remaining parameters as default). Plot the differences between the two cumulative confusion
    matrices of the predictors. Comment.

    \item Considering the unique properties of column\_diagnosis, identify three possible difficulties
    of naïve Bayes when learning from the given dataset.
\end{enumerate}


\end{document}
